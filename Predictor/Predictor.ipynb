{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5422110108570316\n"
     ]
    }
   ],
   "source": [
    "print(random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataSamples(G1,G2,C1,C2,P1,P2,T1,T2):\n",
    "    sampleSize = 100\n",
    "    sample=[]\n",
    "    avg = []\n",
    "    for i in range(sampleSize):\n",
    "        G = round(G1 + (G2-G1)*random(),0)/340\n",
    "        C = round(C1 + (C2-C1)*random(),1)/10.0\n",
    "        P = round(P1 + (P2-P1)*random(),0)/6.0\n",
    "        T = round(T1 + (T2-T1)*random(),0)/120.0\n",
    "        sample.append([G,C,P,T])\n",
    "        G = round(G1 + (G2-G1)*random(),0)\n",
    "        C = round(C1 + (C2-C1)*random(),1)\n",
    "        P = round(P1 + (P2-P1)*random(),0)\n",
    "        T = round(T1 + (T2-T1)*random(),0)\n",
    "        avg.append([G,C,P,T])\n",
    "\n",
    "    return sample,avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[330.0, 9.5, 4.0, 110.0] average\n200\n"
     ]
    }
   ],
   "source": [
    "S1,avg = generateDataSamples(320,340,9,10,1,6,100,120)\n",
    "average = []\n",
    "for i in range(len(avg[0])):\n",
    "    average.append(0)\n",
    "\n",
    "for i in range(len(avg)):\n",
    "    for j in range(len(avg[0])):\n",
    "        average[j] += avg[i][j]\n",
    "for i in range(len(average)):\n",
    "    average[i] = round(average[i]/len(avg),1)\n",
    "average[0] = round(average[0],0)\n",
    "average[2] = round(average[2],0)\n",
    "average[3] = round(average[3],0)\n",
    "print(average,\"average\")\n",
    "S2,avg = generateDataSamples(280,330,6,9,0,6,80,120)\n",
    "train = S1\n",
    "classes = []\n",
    "for i in range(len(S1)):\n",
    "    classes.append(1)\n",
    "for i in range(len(S2)):\n",
    "    classes.append(0)\n",
    "for i in range(len(S2)):\n",
    "    S1.append(S2[i])\n",
    "train=S1\n",
    "print(len(train))\n",
    "#print(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done 0.0\n",
      "Done 1.0\n",
      "Done 2.0\n",
      "Done 3.0\n",
      "Done 4.0\n",
      "Done 5.0\n",
      "Done 6.0\n",
      "Done 7.0\n",
      "Done 8.0\n",
      "Done 9.0\n",
      "Done 10.0\n",
      "Done 11.0\n",
      "Done 12.0\n",
      "Done 13.0\n",
      "Done 14.0\n",
      "Done 15.0\n",
      "Done 16.0\n",
      "Done 17.0\n",
      "Done 18.0\n",
      "Done 19.0\n",
      "Done 20.0\n",
      "Done 21.0\n",
      "Done 22.0\n",
      "Done 23.0\n",
      "Done 24.0\n",
      "Done 25.0\n",
      "Done 26.0\n",
      "Done 27.0\n",
      "Done 28.0\n",
      "Done 29.0\n",
      "Done 30.0\n",
      "Done 31.0\n",
      "Done 32.0\n",
      "Done 33.0\n",
      "Done 34.0\n",
      "Done 35.0\n",
      "Done 36.0\n",
      "Done 37.0\n",
      "Done 38.0\n",
      "Done 39.0\n",
      "Done 40.0\n",
      "Done 41.0\n",
      "Done 42.0\n",
      "Done 43.0\n",
      "Done 44.0\n",
      "Done 45.0\n",
      "Done 46.0\n",
      "Done 47.0\n",
      "Done 48.0\n",
      "Done 49.0\n",
      "Done 50.0\n",
      "Done 51.0\n",
      "Done 52.0\n",
      "Done 53.0\n",
      "Done 54.0\n",
      "Done 55.0\n",
      "Done 56.0\n",
      "Done 57.0\n",
      "Done 58.0\n",
      "Done 59.0\n",
      "Done 60.0\n",
      "Done 61.0\n",
      "Done 62.0\n",
      "Done 63.0\n",
      "Done 64.0\n",
      "Done 65.0\n",
      "Done 66.0\n",
      "Done 67.0\n",
      "Done 68.0\n",
      "Done 69.0\n",
      "Done 70.0\n",
      "Done 71.0\n",
      "Done 72.0\n",
      "Done 73.0\n",
      "Done 74.0\n",
      "Done 75.0\n",
      "Done 76.0\n",
      "Done 77.0\n",
      "Done 78.0\n",
      "Done 79.0\n",
      "Done 80.0\n",
      "Done 81.0\n",
      "Done 82.0\n",
      "Done 83.0\n",
      "Done 84.0\n",
      "Done 85.0\n",
      "Done 86.0\n",
      "Done 87.0\n",
      "Done 88.0\n",
      "Done 89.0\n",
      "Done 90.0\n",
      "Done 91.0\n",
      "Done 92.0\n",
      "Done 93.0\n",
      "Done 94.0\n",
      "Done 95.0\n",
      "Done 96.0\n",
      "Done 97.0\n",
      "Done 98.0\n",
      "Done 99.0\n",
      "0.8878715435835183 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9605332170059335 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8961348051547453 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9835463881938274 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9882250835846735 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8539301977793287 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9066780216242385 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9779364806576033 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9271150775552557 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9382248305807086 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.986342832295599 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9792401730577897 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8854586756996095 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9209538121219241 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9719102038499502 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.841864426550146 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9907285370552881 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8861129685096488 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8908137806566534 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9486678167584367 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8501064565498524 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9810295369350763 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.917182144507382 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9248847035761717 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9842082061547378 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9933551703535694 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.7497736046172018 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.987805961166981 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9673049226446105 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9931595914082609 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.774161732709557 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9939773361279955 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9764558761737997 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9844190399437885 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9740344167523544 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9594434501227777 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9530223282432417 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8357706229215965 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9371515105707203 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9580998691520103 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8979046214426478 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9532394648964524 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9725570211373274 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9794739793574647 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9814297665365196 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9900790848589098 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9470189728581447 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9581523014369201 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9921680988360824 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8884104729343134 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9909674684799042 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.954946244797885 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9919411509250293 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8093579540472983 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9611397578097104 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9327313927736196 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8407796916780005 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.989583186824364 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8454630271227602 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.7944278964832274 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9279731523098855 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.98247213295884 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.862390932111328 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9161963793782798 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9920704533537597 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8900978615529241 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9396119271512834 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9771105291362815 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9725358254151788 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8183521072176704 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9269880715973801 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9866308044720252 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9723276999299224 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.7897900982187324 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.887137921714098 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9551212326068262 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9683876259356857 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9720827798876082 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9528179203143928 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9695252815080203 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8351567463474393 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.848300215830097 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9936061090558918 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.955830156518587 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9791549860305884 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8329061314746926 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9870963154864248 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9022090389185464 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.7072713368430943 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9871968884719446 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9719226053152463 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8440054464409453 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9334252136088946 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9709766053227675 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9467883217871768 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9821793980191206 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9741058084728732 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.87939782872176 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.8699615910683693 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.9431014071132106 val\n",
      "Original class 1 Predicted Class 1\n",
      "0.00012548127964728863 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.2404777862565163 val\n",
      "Original class 0 Predicted Class 0\n",
      "6.772838279302438e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.03840477893192388 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0026432867852645174 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.102484032984795 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.007091235755136069 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.006520415998500365 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.06829164596400565 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.12112467197940673 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0050771295458936555 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.06219513981037252 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.004279325559223971 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0006141720813056321 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00012042571933121814 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.016086003526102812 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.06527704330951174 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0009508403688296009 val\n",
      "Original class 0 Predicted Class 0\n",
      "3.4342749562389436e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0241749809496607 val\n",
      "Original class 0 Predicted Class 0\n",
      "8.220034856962137e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.03360501918932549 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.04998527880872432 val\n",
      "Original class 0 Predicted Class 0\n",
      "8.049917527676352e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.17003401445754374 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.011864462463863207 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0002493529488044018 val\n",
      "Original class 0 Predicted Class 0\n",
      "4.9402308169776006e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.3629960205531873 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0007790221935483856 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00011066213092231549 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.008580736546457433 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.06927385102387278 val\n",
      "Original class 0 Predicted Class 0\n",
      "5.368753091196841e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.002149564145067281 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.014681198495613758 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0034547985637613473 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0016932340249393248 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00040175469030976046 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0006932852343257806 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00021503627778726947 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.007247726246763669 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.17350547937867783 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.20517925108232268 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0009030838043172097 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0001650565282479334 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.05503198115044437 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00012694534631283965 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.000809360771505207 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0009132592328844827 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0001447651401664979 val\n",
      "Original class 0 Predicted Class 0\n",
      "5.162782650980875e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.002799072509982374 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0010876777108283526 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00014078675509416016 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00023004890887225047 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00486133328911106 val\n",
      "Original class 0 Predicted Class 0\n",
      "3.7201877897804825e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0016726938818041291 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0006703571350387715 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.02589906217739634 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0005204871490323224 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0321959775941173 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00014030205806817117 val\n",
      "Original class 0 Predicted Class 0\n",
      "7.700286740185983e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.3781897549023019 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.011040239072406565 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00010892764918476697 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00019710608270347927 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.05271670721115334 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0001833597610493962 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0005059906827192365 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.014201164574427208 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.2817098944635969 val\n",
      "Original class 0 Predicted Class 0\n",
      "9.891838751903646e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0007056845226394856 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.04758144484643651 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.010931216827674914 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.02030476209519279 val\n",
      "Original class 0 Predicted Class 0\n",
      "6.027762223322033e-05 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00309706407691457 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00019888230855518544 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00016825214894051089 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0007133376362637783 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.10854516909192688 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.3080342652265345 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.003347087462301313 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.010504536550542128 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0002916686668065981 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.020041764158403363 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.6131444046872109 val\n",
      "Original class 0 Predicted Class 1\n",
      "0.00010154421599836282 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.15382851609539797 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.0005664115676821414 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.06000109710181966 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.21199190569556486 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.24438428640666635 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00012879491017627078 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.06720843350903605 val\n",
      "Original class 0 Predicted Class 0\n",
      "0.00011005302700730042 val\n",
      "Original class 0 Predicted Class 0\n",
      "Accuracy\n",
      "99.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "l=3 # Choose number of layers here\n",
    "N = [4,3,1] # No of neurons in each layer\n",
    "#train = [[9,9,3],[10,10,3],[9,10,3],[10,9,3],[11,11,3],[10,11,3],[11,10,3],[12,9,3],[9,12,3],[14,10,3],[9,13,3],[10,13,3],[11,8,3],[8,12,3]]\n",
    "#train = [[1,1,3],[-1,-1,3],[-2,-2,3],[-3,-3,3], [2,2,3],[3,3,3],[4,4,3],[5,5,3]]\n",
    "#classes = [0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "#classes = [0,0,0,0,1,1,1,1]\n",
    "\n",
    "#class1 = [[9,9],[10,10],[9,10],[10,9],[11,11],[10,11],[11,10]]\n",
    "#class2 = [[12,9],[9,12],[14,10],[9,13],[10,13],[11,8],[8,12]]\n",
    "#y=[0,0,0,0,1,1,1,1]\n",
    "\"\"\"for i in range(len(train)):\n",
    "    if i<=6:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)\"\"\"\n",
    "\n",
    "w=[]\n",
    "b=[]\n",
    "eta=0.5\n",
    "#adam optimiser parameters\n",
    "alpha = 0.001 # learning rate\n",
    "beta1 = 0.9# parameter for momentum term\n",
    "beta2 = 0.999 # parameter for rms prop\n",
    "epsilon = 10**(-8)\n",
    "Vdw=[]\n",
    "Vdb=[]\n",
    "Sdw=[]\n",
    "Sdb=[]\n",
    "for i in range(0,l-1): # initialising the weights\n",
    "    temp = np.random.randn( N[i+1],N[i] )*0.01\n",
    "    w.append(temp)\n",
    "    temp = np.random.randn( N[i+1],1 ) * 0.01\n",
    "    temp = temp.reshape(-1,1)\n",
    "    b.append(temp)\n",
    "\n",
    "\n",
    "for i in range(0,l-1): # initilaising the paramteres for adam optimiser\n",
    "    temp = np.zeros([ N[i+1],N[i] ],dtype=float)\n",
    "    Vdw.append(temp)\n",
    "    temp = np.zeros( [N[i+1],1 ],dtype=float)\n",
    "    temp=temp.reshape(-1,1)\n",
    "    Vdb.append(temp)\n",
    "    temp = np.zeros( [ N[i+1],N[i] ],dtype=float )\n",
    "    Sdw.append(temp)\n",
    "    temp = np.zeros( [N[i+1],1],dtype=float )\n",
    "    temp = temp.reshape(-1,1)\n",
    "    Sdb.append(temp)\n",
    "\n",
    "\n",
    "trials = 10000 #100000 gives 100 accuracy\n",
    "\n",
    "for trial in range(trials):\n",
    "    \n",
    "    \n",
    "\n",
    "    changeTemp = []\n",
    "    changeBTemp = []\n",
    "    for i in range(0,l-1):\n",
    "        temp = []\n",
    "        for i1 in range(N[i+1]):\n",
    "            temp1 = []\n",
    "            for i2 in range(N[i]):\n",
    "                temp1.append(0)\n",
    "            temp.append(temp1)\n",
    "        tempB = []\n",
    "        for i1 in range(0,N[i+1]):\n",
    "            tempB.append(0)\n",
    "        \n",
    "        temp = np.array(temp)\n",
    "        tempB = np.array(tempB)\n",
    "        tempB = tempB.reshape(-1,1)\n",
    "        changeTemp.append(temp)\n",
    "        changeBTemp.append(tempB)\n",
    "\n",
    "    \n",
    "    \n",
    "    changeMatrix = changeTemp # initialising the gradient matrix for weights\n",
    "    changeBMatrix = changeBTemp # initialsiing gradient matrix for biases\n",
    "\n",
    "\n",
    "    for i in range(0,len(train)): # we are considering the entire training set as a batch\n",
    "        aTemp = []\n",
    "        deltaTemp = []\n",
    "\n",
    "        for p in range(0,l):\n",
    "            tempA = []\n",
    "            tempDelta = []\n",
    "\n",
    "            for j in range(0,N[p]):\n",
    "                tempA.append(0)\n",
    "                tempDelta.append(0)\n",
    "            tempA = np.array(tempA)\n",
    "            tempA = tempA.reshape(-1,1)\n",
    "            tempDelta = np.array(tempDelta)\n",
    "            tempDelta = tempDelta.reshape(-1,1)\n",
    "\n",
    "            aTemp.append(tempA)\n",
    "            deltaTemp.append(tempDelta)\n",
    "\n",
    "        a = aTemp\n",
    "        delta = deltaTemp\n",
    "\n",
    "        \n",
    "        \n",
    "        trainSample = np.array(train[i])\n",
    "        trainSample = trainSample.reshape(-1,1)\n",
    "        a[0] = trainSample\n",
    "        \n",
    "\n",
    "        \n",
    "        for j in range(1,l): #forward Prop\n",
    "            \n",
    "            z = w[j-1].dot( a[j-1]) + b[j-1]\n",
    "            a[j] = (1/(1 + np.exp(-z)))\n",
    "            \n",
    "            \n",
    "        if (classes[i] == 0 and a[l-1][0][0] >0.001 ) or (classes[i] == 1 and a[l-1][0][0] <0.999):# checking if there's error\n",
    "            for j in range(l-1,0,-1):# when there's error, error in every neuron is found using vectorised equations\n",
    "                if j== l-1:\n",
    "                    \n",
    "                    temp = classes[j]\n",
    "                    temp = np.array(classes[j])\n",
    "                    temp = temp.reshape(1,1)\n",
    "                    \n",
    "                    delta[j] =  (a[j] - classes[i]) * a[j] * (1-a[j])\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    delta[j] = w[j].transpose().dot( delta[j+1])\n",
    "                    \n",
    "                    \n",
    "                    temp = np.multiply(a[j],1-a[j])\n",
    "                    delta[j] = np.multiply(delta[j],temp)\n",
    "    \n",
    "        \n",
    "        for j in range(0,l-1):\n",
    "\n",
    "            \n",
    "            \n",
    "            changeMatrix[j] = changeMatrix[j] + delta[j+1].dot( a[j].transpose() ) # udating the gradients of weights here\n",
    "            changeBMatrix[j] = changeBMatrix[j] + delta[j+1] # updating the gradients of biases here\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "    for i in range(0,len(w)):\n",
    "\n",
    "        N1 = 1.0/len(train)\n",
    "        \n",
    "        \n",
    "        changeMatrix[i] = N1*changeMatrix[i]\n",
    "        changeBMatrix[i] =  N1*changeBMatrix[i]\n",
    "        \n",
    "        Vdw[i] = beta1 * Vdw[i] + (1-beta1) * changeMatrix[i]\n",
    "    \n",
    "        Vdb[i] = beta1*Vdb[i] + (1-beta1) * changeBMatrix[i]\n",
    "\n",
    "        \n",
    "        Sdw[i] = beta2*Sdw[i] + (1-beta2) * np.square(changeMatrix[i])\n",
    "    \n",
    "\n",
    "        Sdb[i] = beta2*Sdb[i] + (1-beta2) * np.square(changeBMatrix[i])\n",
    "        \n",
    "        \n",
    "    VdwCorrected = []\n",
    "    VdbCorrected = []\n",
    "    SdwCorrected = []\n",
    "    SdbCorrected = []\n",
    "\n",
    "    for i in range(0,len(w)):\n",
    "        \n",
    "\n",
    "        VdwCorrected.append( Vdw[i]/( 1- beta1**( trial+1 )) )\n",
    "        \n",
    "        VdbCorrected.append( Vdb[i]/( 1- (beta1**(trial+1) ) ) )\n",
    "\n",
    "        SdwCorrected.append( Sdw[i]/( 1- (beta2** (trial+1) ) ) )\n",
    "        SdbCorrected.append( Sdb[i]/( 1- (beta2** (trial+1) )  ) )\n",
    "\n",
    "    \n",
    "    for i in range(0,len(w)):\n",
    "\n",
    "        w[i] = w[i] - alpha * ( VdwCorrected[i]/ ( np.sqrt(SdwCorrected[i]) + epsilon  ) )\n",
    "        b[i] = b[i] - alpha * ( VdbCorrected[i]/ ( np.sqrt(SdbCorrected[i]) + epsilon  ) )\n",
    "\n",
    "    \n",
    "    if (trial % (trials/100) ) == 0:\n",
    "        print(\"Done\", trial/(trials/100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#testing here\n",
    "prop=0\n",
    "for i in range(0,len(train)):\n",
    "    aTemp = []\n",
    "    deltaTemp = []\n",
    "\n",
    "    for p in range(0,l):\n",
    "        tempA = []\n",
    "        tempDelta = []\n",
    "\n",
    "        for j in range(0,N[p]):\n",
    "            tempA.append(0)\n",
    "            tempDelta.append(0)\n",
    "\n",
    "        tempA = np.array(tempA)\n",
    "        tempA = tempA.reshape(-1,1)\n",
    "        tempDelta = np.array(tempDelta)\n",
    "        tempDelta = tempDelta.reshape(-1,1)\n",
    "\n",
    "        aTemp.append(tempA)\n",
    "        deltaTemp.append(tempDelta)\n",
    "\n",
    "    a = aTemp\n",
    "    delta = deltaTemp\n",
    "\n",
    "    trainSample = np.array(train[i])\n",
    "    trainSample = trainSample.reshape(-1,1)\n",
    "    a[0] = trainSample\n",
    "\n",
    "    for j in range(1,l): #forward Prop\n",
    "        \n",
    "        z = w[j-1].dot( a[j-1]) + b[j-1]\n",
    "        a[j] = (1/(1 + np.exp(-z)))\n",
    "    \n",
    "    print(a[l-1][0][0],\"val\")\n",
    "    if a[l-1][0][0] <0.5:\n",
    "        print(\"Original class\",classes[i], \"Predicted Class\",0)\n",
    "        \n",
    "    if a[l-1][0][0] >0.5:\n",
    "        print(\"Original class\",classes[i], \"Predicted Class\",1)\n",
    "    if (classes[i] == 0 and a[l-1][0][0] <0.5 ) or (classes[i] == 1 and a[l-1][0][0] >0.5):\n",
    "        #print(train[i],classes[i])\n",
    "        prop+=1\n",
    "print(\"Accuracy\")\n",
    "print( (float(prop)/len(train))*100 )\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array([[ 3.50289036,  6.20797784,  0.03194757,  0.92241371],\n       [-3.43750174, -6.25863234, -0.03308682, -0.91633425],\n       [-3.51830186, -6.21178742, -0.03175965, -0.92211386]]), array([[ 8.16308201, -8.14460566, -8.16273197]])]\n[array([[-9.50996358],\n       [ 9.48867028],\n       [ 9.52790616]]), array([[3.2986039]])]\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6724950851383608 val\nOriginal class 1 Predicted Class 1\n[0.56113231] [0.43927389] [0.43881982]\n2.937024500414137e-06 val\nOriginal class 1 Predicted Class 0\n[0.01088292] [0.9884274] [0.9892768]\n[array([[ 3.50289036,  6.20797784,  0.03194757,  0.92241371],\n       [-3.43750174, -6.25863234, -0.03308682, -0.91633425],\n       [-3.51830186, -6.21178742, -0.03175965, -0.92211386]]), array([[ 8.16308201, -8.14460566, -8.16273197]])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[array([[-3.43325717, -6.93648135,  0.07977072,  0.27570831],\n",
    "       [ 3.4276012 ,  6.96695906, -0.08034467, -0.27135935],\n",
    "       [ 3.43969576,  6.97135119, -0.08066845, -0.27210358]]), array([[-7.71286476,  7.98071688,  7.97181443]])]\n",
    "[array([[ 9.02020244],\n",
    "       [-9.04630541],\n",
    "       [-9.06110283]]), array([[-4.52417621]])]\"\"\"\n",
    "\n",
    "#testing here\n",
    "prop=0\n",
    "train = [[330/340.0, 9/10.0, 0/6.0,  100/120.0], [  1/340.0, 8/10.0, 3/6.0, 1/120.0]]\n",
    "for i in range(0,len(train)):\n",
    "    aTemp = []\n",
    "    deltaTemp = []\n",
    "\n",
    "    for p in range(0,l):\n",
    "        tempA = []\n",
    "        tempDelta = []\n",
    "\n",
    "        for j in range(0,N[p]):\n",
    "            tempA.append(0)\n",
    "            tempDelta.append(0)\n",
    "\n",
    "        tempA = np.array(tempA)\n",
    "        tempA = tempA.reshape(-1,1)\n",
    "        tempDelta = np.array(tempDelta)\n",
    "        tempDelta = tempDelta.reshape(-1,1)\n",
    "\n",
    "        aTemp.append(tempA)\n",
    "        deltaTemp.append(tempDelta)\n",
    "\n",
    "    a = aTemp\n",
    "    delta = deltaTemp\n",
    "\n",
    "    trainSample = np.array(train[i])\n",
    "    trainSample = trainSample.reshape(-1,1)\n",
    "    a[0] = trainSample\n",
    "\n",
    "    for j in range(1,l): #forward Prop\n",
    "        \n",
    "        z = w[j-1].dot( a[j-1]) + b[j-1]\n",
    "        a[j] = (1/(1 + np.exp(-z)))\n",
    "    \n",
    "    print(a[l-1][0][0],\"val\")\n",
    "    if a[l-1][0][0] <0.5:\n",
    "        print(\"Original class\",classes[i], \"Predicted Class\",0)\n",
    "        print(a[l-2][0],a[l-2][1],a[l-2][2])\n",
    "    if a[l-1][0][0] >0.5:\n",
    "        print(\"Original class\",classes[i], \"Predicted Class\",1)\n",
    "        print(a[l-2][0],a[l-2][1],a[l-2][2])\n",
    "    if (classes[i] == 0 and a[l-1][0][0] <0.5 ) or (classes[i] == 1 and a[l-1][0][0] >0.5):\n",
    "        #print(train[i],classes[i])\n",
    "        prop+=1\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0, 0, 0]\n4\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-0fa85623f133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}